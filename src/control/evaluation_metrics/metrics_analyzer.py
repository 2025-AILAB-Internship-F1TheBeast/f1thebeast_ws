import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse

class MetricsAnalyzer:
    def __init__(self, csv_file_path):
        """
        CSV íŒŒì¼ì—ì„œ metrics ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤
        
        Args:
            csv_file_path (str): CSV íŒŒì¼ ê²½ë¡œ
        """
        self.csv_file_path = csv_file_path
        self.df = None
        self.load_data()
        
    def load_data(self):
        """CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (timestamp 3ì´ˆ ë¯¸ë§Œ ì œì™¸)"""
        try:
            # ì›ë³¸ ë°ì´í„° ë¡œë“œ
            df_raw = pd.read_csv(self.csv_file_path)
            print(f"ğŸ“ ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(df_raw)} ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸")
            
            # timestampê°€ 3ì´ˆ ì´ìƒì¸ ë°ì´í„°ë§Œ í•„í„°ë§
            self.df = df_raw[df_raw['timestamp'] >= 3.0].copy()
            
            # ì¸ë±ìŠ¤ ë¦¬ì…‹
            self.df.reset_index(drop=True, inplace=True)
            
            print(f"âœ… í•„í„°ë§ëœ ë°ì´í„° ë¡œë“œ: {len(self.df)} ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸")
            print(f"ğŸ—‘ï¸  ì œì™¸ëœ ë°ì´í„°: {len(df_raw) - len(self.df)} ê°œ (timestamp < 3.0ì´ˆ)")
            print(f"ğŸ“Š ë¶„ì„ ì‹œê°„ ë²”ìœ„: {self.df['timestamp'].min():.2f}ì´ˆ ~ {self.df['timestamp'].max():.2f}ì´ˆ")
            
            if len(self.df) == 0:
                print("âš ï¸  ê²½ê³ : 3ì´ˆ ì´í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
    def calculate_rms_error(self, column):
        """RMS (Root Mean Square) Error ê³„ì‚°"""
        return np.sqrt(np.mean(self.df[column] ** 2))
    
    def calculate_max_error(self, column):
        """Maximum Absolute Error ê³„ì‚°"""
        return np.max(np.abs(self.df[column]))
    
    def calculate_mean_error(self, column):
        """Mean Absolute Error ê³„ì‚°"""
        return np.mean(np.abs(self.df[column]))
    
    def calculate_std_error(self, column):
        """Standard Deviation ê³„ì‚°"""
        return np.std(self.df[column])
    
    def analyze_metrics(self):
        """ì „ì²´ metrics ë¶„ì„"""
        metrics = ['cross_track_error', 'yaw_error', 'speed_error']
        results = {}
        
        print("\n" + "="*80)
        print("ğŸ” EVALUATION METRICS ANALYSIS")
        print("="*80)
        
        for metric in metrics:
            if metric in self.df.columns:
                rms = self.calculate_rms_error(metric)
                max_err = self.calculate_max_error(metric)
                mean_err = self.calculate_mean_error(metric)
                std_err = self.calculate_std_error(metric)
                
                results[metric] = {
                    'RMS Error': rms,
                    'Max Error': max_err,
                    'Mean Error': mean_err,
                    'Std Deviation': std_err
                }
                
                print(f"\nğŸ“ˆ {metric.upper().replace('_', ' ')}")
                print(f"   RMS Error      : {rms:.6f}")
                print(f"   Max Error      : {max_err:.6f}")
                print(f"   Mean Error     : {mean_err:.6f}")
                print(f"   Std Deviation  : {std_err:.6f}")
                
        return results
    
    def create_summary_table(self, results):
        """ê²°ê³¼ë¥¼ í‘œ í˜•íƒœë¡œ ì •ë¦¬"""
        summary_df = pd.DataFrame(results).T
        summary_df = summary_df.round(6)
        
        print("\n" + "="*80)
        print("ğŸ“‹ SUMMARY TABLE")
        print("="*80)
        print(summary_df.to_string())
        
        # CSVë¡œ ì €ì¥
        output_path = Path(self.csv_file_path).parent / "metrics_summary.csv"
        summary_df.to_csv(output_path)
        print(f"\nğŸ’¾ Summary saved to: {output_path}")
        
        return summary_df
    
    def plot_metrics_over_time(self, save_plots=True):
        """ì‹œê°„ì— ë”°ë¥¸ metrics ë³€í™” í”Œë¡¯"""
        metrics = ['cross_track_error', 'yaw_error', 'speed_error']
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)
        plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(3, 1, figsize=(12, 10))
        fig.suptitle('Performance Metrics Over Time', fontsize=16, fontweight='bold')
        
        colors = ['red', 'blue', 'green']
        labels = ['Cross Track Error (m)', 'Yaw Error (degree)', 'Speed Error (m/s)']
        
        for i, (metric, color, label) in enumerate(zip(metrics, colors, labels)):
            if metric in self.df.columns:
                axes[i].plot(self.df['timestamp'], self.df[metric], 
                           color=color, linewidth=1.5, alpha=0.8)
                axes[i].set_ylabel(label, fontweight='bold')
                axes[i].grid(True, alpha=0.3)
                axes[i].set_title(f'{label} - RMS: {self.calculate_rms_error(metric):.4f}, Max: {self.calculate_max_error(metric):.4f}')
        
        axes[-1].set_xlabel('Time (seconds)', fontweight='bold')
        plt.tight_layout()
        
        if save_plots:
            output_path = Path(self.csv_file_path).parent / "metrics_time_plot.png"
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Time plot saved to: {output_path}")
        
        plt.show()
    
    def plot_metrics_distribution(self, save_plots=True):
        """Metrics ë¶„í¬ íˆìŠ¤í† ê·¸ë¨"""
        metrics = ['cross_track_error', 'yaw_error', 'speed_error']
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        fig.suptitle('Metrics Distribution', fontsize=16, fontweight='bold')
        
        colors = ['red', 'blue', 'green']
        labels = ['Cross Track Error (m)', 'Yaw Error (degree)', 'Speed Error (m/s)']
        
        for i, (metric, color, label) in enumerate(zip(metrics, colors, labels)):
            if metric in self.df.columns:
                axes[i].hist(self.df[metric], bins=50, color=color, alpha=0.7, edgecolor='black', linewidth=0.5)
                axes[i].set_xlabel(label, fontweight='bold')
                axes[i].set_ylabel('Frequency')
                axes[i].grid(True, alpha=0.3)
                axes[i].axvline(self.calculate_mean_error(metric), color='darkred', linestyle='--', 
                              label=f'Mean: {self.calculate_mean_error(metric):.4f}')
                axes[i].legend()
        
        plt.tight_layout()
        
        if save_plots:
            output_path = Path(self.csv_file_path).parent / "metrics_distribution.png"
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Distribution plot saved to: {output_path}")
        
        plt.show()
    
    def plot_trajectory_analysis(self, save_plots=True):
        """ì°¨ëŸ‰ ê¶¤ì ê³¼ target vs actual ë¹„êµ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Trajectory and Performance Analysis', fontsize=16, fontweight='bold')
        
        # 1. ì°¨ëŸ‰ ê¶¤ì 
        scatter = axes[0,0].scatter(self.df['current_x'], self.df['current_y'], 
                                  c=self.df['cross_track_error'], cmap='coolwarm', s=2)
        axes[0,0].set_xlabel('X Position (m)')
        axes[0,0].set_ylabel('Y Position (m)')
        axes[0,0].set_title('Vehicle Trajectory (colored by Cross Track Error)')
        plt.colorbar(scatter, ax=axes[0,0], label='Cross Track Error (m)')
        axes[0,0].grid(True, alpha=0.3)
        axes[0,0].axis('equal')
        
        # 2. ì†ë„ ë¹„êµ
        axes[0,1].plot(self.df['timestamp'], self.df['current_speed'], label='Actual Speed', color='blue', linewidth=1.5)
        axes[0,1].plot(self.df['timestamp'], self.df['target_speed'], label='Target Speed', color='red', linewidth=1.5)
        axes[0,1].set_xlabel('Time (s)')
        axes[0,1].set_ylabel('Speed (m/s)')
        axes[0,1].set_title('Speed Tracking Performance')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. Cross track error ì‹œê°„ ë³€í™”
        axes[1,0].plot(self.df['timestamp'], self.df['cross_track_error'], color='red', linewidth=1.5)
        axes[1,0].set_xlabel('Time (s)')
        axes[1,0].set_ylabel('Cross Track Error (m)')
        axes[1,0].set_title('Cross Track Error Over Time')
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. Yaw error ì‹œê°„ ë³€í™”
        axes[1,1].plot(self.df['timestamp'], np.degrees(self.df['yaw_error']), color='blue', linewidth=1.5)
        axes[1,1].set_xlabel('Time (s)')
        axes[1,1].set_ylabel('Yaw Error (degrees)')
        axes[1,1].set_title('Yaw Error Over Time')
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_plots:
            output_path = Path(self.csv_file_path).parent / "trajectory_analysis.png"
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Trajectory analysis saved to: {output_path}")
        
        plt.show()
    
    def generate_report(self):
        """ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸš€ GENERATING COMPREHENSIVE ANALYSIS REPORT...")
        
        # 1. ê¸°ë³¸ í†µê³„ ë¶„ì„
        results = self.analyze_metrics()
        
        # 2. ìš”ì•½ í‘œ ìƒì„±
        summary_df = self.create_summary_table(results)
        
        # 3. í”Œë¡¯ ìƒì„±
        self.plot_metrics_over_time()
        self.plot_metrics_distribution()
        self.plot_trajectory_analysis()
        
        # 4. ì„±ëŠ¥ í‰ê°€
        self.evaluate_performance(results)
        
        return results, summary_df
    
    def evaluate_performance(self, results):
        """ì„±ëŠ¥ í‰ê°€ ë° ë“±ê¸‰ ë§¤ê¸°ê¸°"""
        print("\n" + "="*80)
        print("ğŸ¯ PERFORMANCE EVALUATION")
        print("="*80)
        
        # ì„±ëŠ¥ ê¸°ì¤€ (ì˜ˆì‹œ)
        thresholds = {
            'cross_track_error': {'excellent': 0.05, 'good': 0.1, 'fair': 0.2},
            'yaw_error': {'excellent': 0.05, 'good': 0.1, 'fair': 0.2},
            'speed_error': {'excellent': 0.5, 'good': 1.0, 'fair': 2.0}
        }
        
        def get_grade(error, thresholds):
            if error <= thresholds['excellent']:
                return "ğŸ† Excellent"
            elif error <= thresholds['good']:
                return "ğŸ¥ˆ Good"
            elif error <= thresholds['fair']:
                return "ğŸ¥‰ Fair"
            else:
                return "âŒ Needs Improvement"
        
        for metric, values in results.items():
            rms_error = values['RMS Error']
            max_error = values['Max Error']
            
            if metric in thresholds:
                rms_grade = get_grade(rms_error, thresholds[metric])
                max_grade = get_grade(max_error, thresholds[metric])
                
                print(f"\nğŸ“Š {metric.upper().replace('_', ' ')}")
                print(f"   RMS Error Grade: {rms_grade} ({rms_error:.6f})")
                print(f"   Max Error Grade: {max_grade} ({max_error:.6f})")

def main():
    parser = argparse.ArgumentParser(description='Analyze F1Tenth racing metrics')
    parser.add_argument('csv_file', help='Path to the metrics CSV file')
    parser.add_argument('--no-plots', action='store_true', help='Skip generating plots')
    
    args = parser.parse_args()
    
    # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
    csv_path = Path(args.csv_file)
    if not csv_path.exists():
        print(f"âŒ Error: File {csv_path} not found!")
        return
    
    # ë¶„ì„ê¸° ìƒì„± ë° ì‹¤í–‰
    analyzer = MetricsAnalyzer(str(csv_path))
    
    if args.no_plots:
        results = analyzer.analyze_metrics()
        analyzer.create_summary_table(results)
        analyzer.evaluate_performance(results)
    else:
        analyzer.generate_report()

if __name__ == "__main__":
    main()